{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.filters import roberts\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import clear_border\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "import cv2\n",
    "import skimage\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kaggle(ct_scan_loc):\n",
    "    # get image and pull origin and spacing for translating nodule coordinates from file\n",
    "    ct_scan = Image.open(ct_scan_loc)\n",
    "    # ct_scan = Image.open(\"cancer_data\\Bengin cases\\Bengin case (\" + str(x) + \").jpg\")\n",
    "    ct_scan = ImageOps.grayscale(ct_scan)\n",
    "    ct_scan = np.array(ct_scan)\n",
    "    # blur image\n",
    "    # ct_scan_thresh_mask = gaussian_filter(ct_scan, sigma=(1, 1))\n",
    "    ct_scan_thresh_mask = ct_scan\n",
    "    # ct_scan_thresh_mask = ct_scan_thresh_mask < 175\n",
    "    # x+=1\n",
    "\n",
    "    # not in hounsfield units\n",
    "    # range of tissue and bronchioles to keep: study doesn't give exact number (close to -1000 or above -320 masked)\n",
    "    min_hu = 100 #100\n",
    "    max_hu = 175 #175\n",
    "    ct_scan_thresh_mask = np.clip(ct_scan_thresh_mask, min_hu, max_hu)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    ##https://www.kaggle.com/code/arnavkj95/candidate-generation-and-luna16-preprocessing/notebook########\n",
    "    ##### preprocessing ideas taken from the above website ###############################################\n",
    "    #######################################################################################################\n",
    "    ct_scan_thresh_mask = clear_border(ct_scan_thresh_mask)\n",
    "    #######################################################################################################\n",
    "    ##### End preprocesing ideas########################### ###############################################\n",
    "    #######################################################################################################\n",
    "    \n",
    "    # set paramters for blob detector\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "    params.filterByInertia = False\n",
    "    params.filterByConvexity = False\n",
    "    params.filterByColor = True\n",
    "    params.blobColor = 255\n",
    "    params.filterByArea = True\n",
    "    params.maxArea = 1000\n",
    "    params.minArea = 0\n",
    "\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    ct_scan_thresh_mask = cv2.erode(ct_scan_thresh_mask,kernel,iterations = 1)\n",
    "    \n",
    "    #detect blobs\n",
    "    detector = cv2.SimpleBlobDetector_create(params)\n",
    "    keypoints = detector.detect(ct_scan_thresh_mask)\n",
    "    im_with_keypoints = cv2.drawKeypoints(ct_scan_thresh_mask, keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imshow(\"Keypoints\", im_with_keypoints)\n",
    "    \n",
    "    #obtain keypoints and diameters\n",
    "    keypoint_list = []\n",
    "    for i in keypoints:\n",
    "        x_loc = i.pt[0]\n",
    "        y_loc = i.pt[1]\n",
    "        size = i.size #diameter of blob\n",
    "        keypoint_list.append([x_loc, y_loc,size])\n",
    "    # will use format for boxes im_with_keypoints[ymin:ymax, xmin:xmax]\n",
    "    # set boxes to 1\n",
    "    zeros = np.zeros(ct_scan_thresh_mask.shape)\n",
    "    for i in keypoint_list:\n",
    "        range_val = np.ceil(i[2])\n",
    "        zeros[int(i[1]-range_val):int(i[1]+range_val), int(i[0]-range_val):int(i[0]+range_val)] = 1\n",
    "\n",
    "    # assign masked regions to original ct\n",
    "    ct_scan[zeros == 0] = 0\n",
    "    ct_scan[ct_scan < 110] = 0\n",
    "\n",
    "    # Need to normal 0-1 and zero_center\n",
    "    ct_scan = minmax_scale(ct_scan, feature_range=(0, 1))\n",
    "\n",
    "    # 3d spline interpolation to 0.5 in xyz directions\n",
    "    ct_scan = resize(ct_scan, (256, 256), mode=\"reflect\")\n",
    "\n",
    "    # zero mean to make data symmetric\n",
    "    meanCenter = lambda x: x - x.mean()\n",
    "    ct_scan = meanCenter(ct_scan)\n",
    "    return ct_scan\n",
    "\n",
    "\n",
    "def top_nodules(image):\n",
    "    # 32x32 window\n",
    "    #get sums of windows\n",
    "    image = resize(image, (256, 256), mode=\"reflect\")\n",
    "    min_x, max_x=0,32\n",
    "    sums = []\n",
    "    while max_x < image.shape[0]:\n",
    "        min_y, max_y = 0,32\n",
    "        while max_y < image.shape[1]:\n",
    "            sum_window = sum(sum(image[min_x:max_x,min_y:max_y]))[0]\n",
    "            sums.append([sum_window, min_x, max_x, min_y, max_y])\n",
    "            min_y+=1\n",
    "            max_y+=1\n",
    "        min_x+=1\n",
    "        max_x+=1\n",
    "    # sort by sum\n",
    "    sums = sorted(sums, reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    #get only top sums that don't overlap\n",
    "    final_sums = []\n",
    "    for i in sums:\n",
    "        if len(final_sums) < 4:\n",
    "            if len(final_sums)==0:\n",
    "                final_sums.append(i)\n",
    "                continue\n",
    "            else:\n",
    "                included = 0\n",
    "                for j in final_sums:\n",
    "                    if (abs(i[1] - j[1]) < 32) & (abs(i[3] - j[3]) < 32):\n",
    "                        included = 1\n",
    "                if included == 0:\n",
    "                    final_sums.append(i)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    #concatentate top 4 windows\n",
    "    cancer_input = np.concatenate((image[final_sums[0][1]:final_sums[0][2],final_sums[0][3]:final_sums[0][4]],image[final_sums[1][1]:final_sums[1][2],final_sums[1][3]:final_sums[1][4]]), axis=1)\n",
    "    cancer_input2 = np.concatenate((image[final_sums[2][1]:final_sums[2][2],final_sums[2][3]:final_sums[2][4]],image[final_sums[3][1]:final_sums[3][2],final_sums[3][3]:final_sums[3][4]]), axis=1)\n",
    "    cancer_input_concat = np.concatenate((cancer_input, cancer_input2), axis=0)\n",
    "    return cancer_input_concat\n",
    "    \n",
    "    \n",
    "def train_model(X_train, y_train):\n",
    "    #############################################################################################################################################################\n",
    "    ############### Code mostly taken from DigitalSreeni at https://www.youtube.com/watch?v=GAYJ81M58y8 ################################################################\n",
    "    ############### Using this code as it matches the U-Net model needed, with updates to parameters and the addition of dropout layers #################\n",
    "    ##############################################################################################################################################################\n",
    "    from tensorflow.keras.utils import normalize\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from keras.layers import (\n",
    "        Conv2D,\n",
    "        MaxPooling2D,\n",
    "        Input,\n",
    "        UpSampling2D,\n",
    "        Dropout,\n",
    "        concatenate,\n",
    "        Dense,\n",
    "        Flatten\n",
    "    )\n",
    "    from keras.models import Model\n",
    "\n",
    "    # convolutions\n",
    "    inputs = Input((64, 64, 1))\n",
    "    s1 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    s2 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(s1)\n",
    "    p1 = MaxPooling2D(pool_size=(2, 2))(s2)\n",
    "    s3 = Dropout(0.2)(p1)\n",
    "    s4 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(s2)\n",
    "    p2 = MaxPooling2D(pool_size=(2, 2))(s4)\n",
    "    s5 = Dropout(0.2)(p2)\n",
    "    s6 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(s5)\n",
    "    s7 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(s6)\n",
    "    f1 = Flatten()(s7)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(f1)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"cancer_model\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    ############################################################################################\n",
    "    ########## end code taken from DigitalSreeni ###############################################\n",
    "    ###################### at https://www.youtube.com/watch?v=GAYJ81M58y8#######################\n",
    "    ############################################################################################\n",
    "\n",
    "    # get top 8 suspicious non-overlapping regions for cancer/non-cancer\n",
    "\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "    # fit model and show results of each epoch\n",
    "    checkpoint_name = \"Weights-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        checkpoint_name, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\"\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "    #train model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=5, callbacks=callbacks_list, validation_split=0.1,)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodule candidates ready for training\n"
     ]
    }
   ],
   "source": [
    "#get all images adn save their labels\n",
    "folders = [\n",
    "        \"cancer_data/Bengin cases/\",\n",
    "        \"cancer_data/Malignant cases/\",\n",
    "    ]\n",
    "images=[]\n",
    "for folder in folders:\n",
    "    print('checking folder: ', folder)\n",
    "    files = os.listdir(folder)\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        image_loc = str(folder) + str(file)\n",
    "        image = preprocess_kaggle(image_loc)\n",
    "        if folder == \"cancer_data/Bengin cases/\":\n",
    "            images.append([image, 0])\n",
    "        else:\n",
    "            images.append([image, 1])\n",
    "print(\"images imported\")\n",
    "\n",
    "# import model\n",
    "model = tf.keras.models.load_model('model_0503TF_2')\n",
    "print(\"model imported\")\n",
    "\n",
    "#get list of all input images and get nodules       \n",
    "input_images = [i[0] for i in images]\n",
    "np_input = np.array(input_images)\n",
    "np_input = np_input.reshape(len(np_input), 256, 256, 1)\n",
    "pred_NN = model.predict(np_input)\n",
    "print(\"nodule candidates found\")\n",
    "\n",
    "#get all nodules\n",
    "quad_nodules = []\n",
    "x=0\n",
    "while x < len(pred_NN):\n",
    "    quad = top_nodules(pred_NN[x])\n",
    "    cancer = images[x][1]\n",
    "    quad_nodules.append([quad, cancer])\n",
    "    x+=1\n",
    "print(\"nodule candidates ready for training\")\n",
    "\n",
    "\n",
    "# separate into train and test sets\n",
    "np.random.shuffle(quad_nodules)\n",
    "test_set = []\n",
    "no=0\n",
    "yes=0\n",
    "while (yes < 110) | (no < 40):\n",
    "    if yes < 110:\n",
    "        x=0\n",
    "        while x < len(quad_nodules):\n",
    "            if quad_nodules[x][1] == 1:\n",
    "                pop = quad_nodules.pop(x)\n",
    "                test_set.append(pop)\n",
    "                yes+=1\n",
    "                break\n",
    "            x+=1\n",
    "    if no < 40:\n",
    "        x=0\n",
    "        while x < len(quad_nodules):\n",
    "            if quad_nodules[x][1] == 0:\n",
    "                pop = quad_nodules.pop(x)\n",
    "                test_set.append(pop)\n",
    "                no+=1\n",
    "                break\n",
    "            x+=1\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for use later in case needed\n",
    "filename = \"quad_nodules.sav\"\n",
    "pickle.dump(quad_nodules, open(filename, \"wb\"))\n",
    "filename = \"test_set_nodules.sav\"\n",
    "pickle.dump(test_set, open(filename, \"wb\"))\n",
    "\n",
    "# get input and outputs\n",
    "quad_nodules_x = [i[0] for i in quad_nodules]\n",
    "quad_nodules_y = [i[1] for i in quad_nodules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cancer_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 64, 64, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 64, 64, 64)        640       \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 32, 32, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 32, 32, 64)        73792     \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 287,681\n",
      "Trainable params: 287,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.8491\n",
      "Epoch 1: val_loss improved from inf to 0.50365, saving model to Weights-001--0.50365.hdf5\n",
      "96/96 [==============================] - 2s 18ms/step - loss: 0.4508 - accuracy: 0.8491 - val_loss: 0.5036 - val_accuracy: 0.7778\n",
      "Epoch 2/10\n",
      "94/96 [============================>.] - ETA: 0s - loss: 0.3923 - accuracy: 0.8574\n",
      "Epoch 2: val_loss did not improve from 0.50365\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3923 - accuracy: 0.8574 - val_loss: 0.5380 - val_accuracy: 0.7778\n",
      "Epoch 3/10\n",
      "89/96 [==========================>...] - ETA: 0s - loss: 0.4062 - accuracy: 0.8494\n",
      "Epoch 3: val_loss improved from 0.50365 to 0.48614, saving model to Weights-003--0.48614.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3909 - accuracy: 0.8574 - val_loss: 0.4861 - val_accuracy: 0.7778\n",
      "Epoch 4/10\n",
      "89/96 [==========================>...] - ETA: 0s - loss: 0.3745 - accuracy: 0.8562\n",
      "Epoch 4: val_loss improved from 0.48614 to 0.44307, saving model to Weights-004--0.44307.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3706 - accuracy: 0.8574 - val_loss: 0.4431 - val_accuracy: 0.7778\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3552 - accuracy: 0.8553\n",
      "Epoch 5: val_loss improved from 0.44307 to 0.41417, saving model to Weights-005--0.41417.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3552 - accuracy: 0.8553 - val_loss: 0.4142 - val_accuracy: 0.7778\n",
      "Epoch 6/10\n",
      "89/96 [==========================>...] - ETA: 0s - loss: 0.3252 - accuracy: 0.8674\n",
      "Epoch 6: val_loss improved from 0.41417 to 0.40317, saving model to Weights-006--0.40317.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3278 - accuracy: 0.8637 - val_loss: 0.4032 - val_accuracy: 0.8148\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8595\n",
      "Epoch 7: val_loss improved from 0.40317 to 0.39388, saving model to Weights-007--0.39388.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3230 - accuracy: 0.8595 - val_loss: 0.3939 - val_accuracy: 0.8148\n",
      "Epoch 8/10\n",
      "89/96 [==========================>...] - ETA: 0s - loss: 0.3146 - accuracy: 0.8584\n",
      "Epoch 8: val_loss improved from 0.39388 to 0.36491, saving model to Weights-008--0.36491.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3123 - accuracy: 0.8595 - val_loss: 0.3649 - val_accuracy: 0.8148\n",
      "Epoch 9/10\n",
      "89/96 [==========================>...] - ETA: 0s - loss: 0.2892 - accuracy: 0.8697\n",
      "Epoch 9: val_loss improved from 0.36491 to 0.35986, saving model to Weights-009--0.35986.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2856 - accuracy: 0.8742 - val_loss: 0.3599 - val_accuracy: 0.8519\n",
      "Epoch 10/10\n",
      "89/96 [==========================>...] - ETA: 0s - loss: 0.2689 - accuracy: 0.8787\n",
      "Epoch 10: val_loss improved from 0.35986 to 0.34925, saving model to Weights-010--0.34925.hdf5\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2698 - accuracy: 0.8763 - val_loss: 0.3492 - val_accuracy: 0.8519\n"
     ]
    }
   ],
   "source": [
    "#convert to numpu arrays and train model\n",
    "quad_nodules_x = np.array(quad_nodules_x)\n",
    "# quad_nodules_x = np_input.reshape(len(quad_nodules_x), 64, 64, 1)\n",
    "quad_nodules_y = np.array(quad_nodules_y)\n",
    "# quad_nodules_y = np_input.reshape(len(quad_nodules_y), 1)\n",
    "cancer_model = train_model(quad_nodules_x, quad_nodules_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test set\n",
    "# get input and outputs\n",
    "test_nodules_x = [i[0] for i in test_set]\n",
    "test_nodules_y = [i[1] for i in test_set]\n",
    "\n",
    "#predict on test set\n",
    "test_nodules_x = np.array(test_nodules_x)\n",
    "test_results = cancer_model.predict(test_nodules_x)\n",
    "\n",
    "# save test set to a dataframe\n",
    "result_df = pd.DataFrame(test_results)\n",
    "result_df.columns=['probability']\n",
    "result_df['actual'] = test_nodules_y\n",
    "\n",
    "# chose 80% as cutoff for prediction - could move either way to increase/decrease FP and FN\n",
    "result_df['prediction'] = 0\n",
    "result_df.loc[result_df['probability']>=0.8,'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6733333333333333\n",
      "Precision:  0.8426966292134831\n",
      "Recall:  0.6818181818181818\n",
      "F1:  0.7537688442211055\n",
      "False Negative:  0.3181818181818182\n",
      "False Positive:  0.35\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "accuracy = len(result_df[result_df['prediction'] == result_df['actual']]) / len(result_df)\n",
    "tp = len(result_df[(result_df['prediction'] == 1) & (result_df['actual'] == 1)])\n",
    "precision = tp / (tp + len(result_df[(result_df['prediction'] == 1) & (result_df['actual'] == 0)]))\n",
    "tp = len(result_df[(result_df['prediction'] == 1) & (result_df['actual'] == 1)])\n",
    "recall = tp / (tp + len(result_df[(result_df['prediction'] == 0) & (result_df['actual'] == 1)]))\n",
    "#use f1 since classes are of different sizes\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "fn =  len(result_df[(result_df['prediction'] == 0) & (result_df['actual'] == 1)]) \n",
    "fnr = fn / (fn + len(result_df[(result_df['prediction'] == 1) & (result_df['actual'] == 1)]) )\n",
    "fp =  len(result_df[(result_df['prediction'] == 1) & (result_df['actual'] == 0)]) \n",
    "fpr = fp / (fp + len(result_df[(result_df['prediction'] == 0) & (result_df['actual'] == 0)]) ) \n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision) \n",
    "print('Recall: ', recall)     \n",
    "print('F1: ', f1)\n",
    "print('False Negative: ', fnr)\n",
    "print('False Positive: ', fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  2.,  4.,  7.,  8.,\n",
       "         2.,  9.,  7., 10.,  8., 10., 41.]),\n",
       " array([0.15464914, 0.19691667, 0.23918422, 0.28145176, 0.3237193 ,\n",
       "        0.36598682, 0.40825436, 0.4505219 , 0.49278945, 0.53505695,\n",
       "        0.5773245 , 0.6195921 , 0.6618596 , 0.70412713, 0.74639463,\n",
       "        0.7886622 , 0.83092976, 0.87319726, 0.9154648 , 0.9577323 ,\n",
       "        0.9999999 ], dtype=float32),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(result_df['probability'][result_df['actual'] == 1], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 2., 4., 5., 7., 3., 3., 1.,\n",
       "        1., 2., 8.]),\n",
       " array([0.12021066, 0.16420005, 0.20818946, 0.25217885, 0.29616824,\n",
       "        0.34015763, 0.38414705, 0.42813644, 0.47212583, 0.51611525,\n",
       "        0.5601046 , 0.604094  , 0.6480834 , 0.6920728 , 0.73606217,\n",
       "        0.7800516 , 0.824041  , 0.86803037, 0.9120198 , 0.95600915,\n",
       "        0.99999857], dtype=float32),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(result_df['probability'][result_df['actual'] == 0], bins=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
